---
title: "Verfügbare Modelle abrufen"
api: "GET https://app.famulor.de/api/user/assistants/models"
icon: "brain-circuit"
description: "Verfügbare KI-Modelle für die Assistenten-Konfiguration abrufen"
---

Dieser Endpunkt gibt eine Liste der verfügbaren KI-Modelle zurück, die beim Erstellen oder Aktualisieren von Assistenten verwendet werden können.

### Query-Parameter

<ParamField query="type" type="string" default="llm">
  Der Typ der abzurufenden Modelle, basierend auf dem Engine-Modus:

  * `llm` - LLM-Modelle für den Modus **pipeline** (Standard)
  * `multimodal` - Multimodale Modelle für den Modus **multimodal**
  * `dualplex` - Multimodale Modelle für den Modus **dualplex**
</ParamField>

### Antwort-Felder

<ResponseField name="data" type="array">
  <Expandable title="Eigenschaften">
    <ResponseField name="id" type="integer">
      Die eindeutige Kennung des Modells
    </ResponseField>

    <ResponseField name="name" type="string">
      Der Anzeigename des Modells
    </ResponseField>

    <ResponseField name="code" type="string">
      Der interne Code (nur für multimodal/dualplex Modelle)
    </ResponseField>
  </Expandable>
</ResponseField>

<ResponseExample>
  ```json LLM Modelle (type=llm) theme={null}
  [
    {
      "id": 1,
      "name": "GPT-4o-mini"
    },
    {
      "id": 2,
      "name": "GPT-4.1-mini"
    }
  ]
  ```

  ```json Multimodal/Dualplex Modelle (type=multimodal oder type=dualplex) theme={null}
  [
    {
      "id": 1,
      "name": "GPT-4o",
      "code": "gpt-4o-realtime"
    },
    {
      "id": 4,
      "name": "GPT Realtime",
      "code": "gpt-realtime"
    }
  ]
  ```
</ResponseExample>

### Hinweise

* Wenn kein `type`-Parameter angegeben wird, werden standardmäßig LLM-Modelle zurückgegeben
* Verwenden Sie das Feld `llm_model_id`, wenn Sie **pipeline** Assistenten erstellen
* Verwenden Sie das Feld `multimodal_model_id`, wenn Sie **multimodal** oder **dualplex** Assistenten erstellen
